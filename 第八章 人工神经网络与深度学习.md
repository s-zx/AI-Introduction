# 第八章 人工神经网络与深度学习
人工神经网络是一个用大量_简单处理单元_经过广泛连接而组成的人工网络。人工神经网络为许多问题的研究提供了新的思路，特别是深度学习，能够发现_高维数据_中的**复杂结构**，取得比机器学习更好的效果。
## 神经元与神经网络
### 生物神经元结构
神经元的主体部分为细胞体，细胞体由细胞核、细胞质、细胞膜等组成。神经元还包括树突和一条长的轴突，轴突末端部分有很多分枝称为轴突末梢。
轴突用来传递和输出信息，其末端的许多轴突末梢为信号输出端子，将神经冲动传给其他神经元。
轴突相当于细胞的输入端，树突的全长各点都能接收其他神经元的冲动，神经冲动只能由前一级神经元的轴突末梢传向下一级神经元的**树突或细胞体**，_不能反向传递_。
神经元具有两种常规工作状态—兴奋与抑制，当传入的神经冲动使细胞膜电位升高超过阈值时，细胞进入兴奋状态，产生神经冲动并由轴突输出；当传入的神经冲动使膜电位下降低于阈值时，细胞进入抑制状态，没有神经冲动输出。
### 神经元数学模型
神经元的一种所谓标准、统一的数学模型由三部分组成，即加权求和、线性动态系统和非线性函数映射，如下图所示。
![](https://tva1.sinaimg.cn/large/008i3skNgy1gz9g4oy0g0j30u0140dli.jpg)
加权求和是把其他神经元的输出对其第i个神经元的作用以及外部输出的作用求和，再减去其阈值。
线性环节最常见的是比例函数，而最常用的非线性激励函数有如下几种
![](https://tva1.sinaimg.cn/large/008i3skNgy1gz9g4tkfbvj31400u0afm.jpg)
  ReLU（rectified linear units)函数是近年来深度学习中广泛使用的一个激活函数，他不是一个光滑曲线，而是一个很简单的分段线性函数，如下所示：  
![](https://tva1.sinaimg.cn/large/008i3skNgy1gz9g4wr6f8j30u014079c.jpg)
ReLU函数尽管形式简单，但在实际应用中没有饱和问题，运算速度快，收敛效果好，在卷积神经网络等深度神经网络中效果很好。

### 神经网络的结构
神经网络是由众多简单的神经元的轴突和其他神经元或者自身的树突相连接而组成的一个网络。
根据神经网络中神经元的连接方式，可划分为不同类型的结构，目前，人工神经网络主要有前馈型和反馈型两大类神经网络。
前馈型：个神经元接受前一层的输入并输出给下一层，没有反馈。BP神经网络、卷积神经网络都是前馈型神经网络。
反馈型：存在一些神经元输出经过若干个神经元后，再反馈到这些神经元的输入端。最典型的是Hopfield神经网络，它是全互联神经网路，即每个神经元和其他神经元都相连。
### 神经网络的学习
神经网络方法是一种知识表示方法和推理方法，产生式、框架等方法 是知识的显式表示，而神经网络知识表示是一种_隐式的表示方法_，它将某一问题的若干知识通过学习表示在同一网络中。
神经网络的学习是指调整神经网络的_连接权值或者结构_，使输入和输出具有重要的特性。

## BP神经网络及其学习算法
### BP神经网络的结构
BP神经网络（back-propagation neural network)是多层前向网络，其结构如图：
![](https://tva1.sinaimg.cn/large/008i3skNgy1gz9g50lfrcj30u0140wl6.jpg)
设BP神经网络具有m层，第一层称为输入层，最后一层称为输出层，中间各层称为**隐层**。标上“+1”的圆圈称为偏置节点，没有其他单元连向偏执单元，偏置单元没有输入，它的输出总是+1.
输入层起缓冲存储器的作用，把数据源加到网络上，因此输入层的神经元的输入输出关系一般是_线性函数_。隐层中各个神经元的输入输出关系一般为非线性函数。
BP神经网络可以看做是一个从输入到输出的非线性映射。
BP神经网络具有很强的学习能力，但对于多层BP神经网络，如何合理地选取BP网络的隐层数以及隐层的节点数，目前尚无有效的理论和方法。现在的问题是如何调整神经网络的权值，使BP神经网络输入和输出之间的关系与给定的样本相同？BP学习算法给出了答案。

### BP学习算法
给定N组输入输出样本为Xi,Yi,如何调整BP神经网络的权值，使BP神经网络输入为样本Xi时，神经网络输出为Yi，这就是BP神经网络的学习问题。可见_BP学习算法是一种有监督学习_。
BP学习算法通过**反向学习**过程使误差最小，即选择神经网络权值使期望输出与神经网络实际输出之差的平方和最小。这种学习算法实际上是求目标函数的极小值，可以利用非线性规划中的_最快下降法_使权值沿目标函数的负梯度方向改变。
神经元的非线性函数一般取为S型函数，可以推导出下列BP学习算法 ：
![](https://tva1.sinaimg.cn/large/008i3skNgy1gz9g54wmxnj30u0140qa8.jpg)，从公式可以看出，求第k层的误差信号需要d(k)需要上一层的d(k+1)，因此误差函数的求取是一个始于**输出层**的反向传播的递归过程，所以又称反向传播学习算法。

### BP学习算法的实现
BP学习算法的程序框图如下所示：![](https://tva1.sinaimg.cn/large/008i3skNgy1gz9g58mrmoj30u0140440.jpg)
在BP算法实现时，还要主义下列问题：

1. 训练数据预处理。预处理过程包含将所有的特征变换为【0，1】或者【-1，1】区间内，使得在每个训练集上每个特征的均值为0且具有相同的方差。
2. 后处理。当应用神经网络进行分类操作时，通常将输出值_编码_成所谓的**名义变量**，具体的值对应类别标号。
3. 初始权值的设置。和所有梯度下降算法一样，初始权值对BP神经网络的最终解有很大的影响。虽然全部设置为0显得比较自然，但是会导致不理想的结果。如果输出层的权值全部为0，则反向传播的误差也将为0，输出层前面的权值将不会改变。因此，一般以一个均值为0的随机分布设置BP神经网络的初始权值。

## 卷积神经网络
由于BP学习算法具有收敛速度慢、需要大量带标签的训练数据、容易陷入局部最优等缺点，因此BP神经网络只能包含少许隐层，从而限制了BP学习算法的性能，影响了该算法在诸多工程领域中的应用。许多研究通过数学和工程技巧来_增加神经网络隐层的层数_也就是**深度**，这样的神经网络称为深度神经网络。
### 人脑视觉机理
机器学习是研究计算机模拟人类学习行为的学科，因此，我们先了解人的视觉系统是怎么工作的，怎么知道哪些特征好、哪些特征不好。
 1958年，David Hubel发现了一种被称为“方向选择性细胞”的神经元，当瞳孔发现物体的边缘，而且这个边缘指向某个方向时，这种神经元就会兴奋。因此，神经-中枢-大脑的工作过程或许是一个不断迭代、不断抽象的过程，即从原始信号做低级抽象，逐渐向高级抽象迭代。
1981年诺贝尔医学奖获得者David Hubel发现视觉系统的信息处理是分级的--从低级的V1区提取**边缘特征**，再到V2区的_形状或者目标的部分_等，再到更高层_整个目标、目标的行为_等。也就是说，_高层特征是底层特征的组合_，从低层到高层的特征表示越来越抽象、越来越能表现语义或者意图。而抽象层面越高，存在的可能猜测就越少，越利于分类。
1989年，Yann LeCun受生物学发现的启发，提出了卷积神经网络（convolutional neural networks,CNN)。CNN更像生物神经网络，是深度学习的基础。在模式分类领域，由于该网络避免了对图像的复杂前期预处理，可以直接输入原始图像，避免了传统识别算法中复杂的特征提取和数据重建过程，因而得到了更广泛的应用，成为语音分析和图像识别领域的研究重点。
### 卷积神经网络的结构
卷积神经网络是一种多层神经网络，每层由多个二维平面组成，而每个二维平面由多个独立神经元组成，如图所示：![](https://tva1.sinaimg.cn/large/008i3skNgy1gz9g5cbwmcj30u0140ahq.jpg)
输入层通常是一个矩阵，例如一幅图像的像素组成的矩阵。
C层为特征提取层，称为卷积层，对输入图像进行卷积，_提取该局部的特征_。
S层是特征映射层，称为池化层或者下采样层，对提取的局部特征进行_综合_。网络的每个计算层由多个特征映射组成，每个特征映射为一个平面，平面上所有神经元的权值相等。特征映射结构采用sigmoid函数作为激活函数，_使特征映射具有位移不变性。_
CNN中的每一个卷积层C都紧跟着一个求局部平均用于**二次提取特征**的池化层S。C、S层中的每一层都由多个二维平面i组成，每一个二维平面是一个**特征图**。这种特有的两次特征提取结构能够容许识别过程中**输入样本有较严重的畸变**。
一个典型的卷积神经网络由一系列的过程组成，最初的几个阶段是由卷积层和池化层组成，卷积层的单元被组织在特征图中。在特征图中，每一个单元通过一组称为滤波器的权值被连接到上一层特征图的一个局部块，然后这个局部加权和被传给一个非线性函数。
一个特征图中的全部单元享用**相同的滤波器**，不同层的特征图使用不同的滤波器，使用这种结构有两个原因：一是在数组数据中，附近的值经常是高度相关的，具有明显的**局部特征**；二是在一个地方出现的某个特征也可能出现在其他地方，所以不同位置的单元可以**共享权值**。在数学上，这种由一个特征图执行的滤波操作是一个**离线卷积**，这也是卷积神经网络名称的由来。

### 卷积神经网络的卷积运算
卷积是数学上的一个重要运算，由于其具有丰富的物理、生物、生态等意义，所以具有非常广泛的应用。下面介绍卷积神经网络中用到的卷积运算方法。
在图像处理中，采用卷积运算对_输入图像或CNN上一层的特征图_进行变换，也就是**特征提取**，以得到新的特征。这就是为什么卷积之后的结果被称为**特征图**。
一副灰度图片可以用一个像素矩阵表示，矩阵中的每个数字的取值范围为【0，255】，0表示黑色，255表示白色，其他灰度为介于0到255之间的整数。如果是彩色图片，则用RGB三个像素矩阵共同表示，每个像素矩阵称为**通道**，因此灰度图像为单通道，彩色图像为三通道。
对于计算机而言，提取图像的特征实际上就是对数字矩阵进行运算，其中最重要的运算就是**卷积**。
为计算简单起见，给定一个5✖️5的像素值的矩阵，像素值为0或1；卷积核是一个3×3的矩阵，其值为0或1。
用卷积核矩阵在输入矩阵上从左到右、从上到下滑动，每次滑动s个像素，滑动的距离s称为**步幅**。卷积特征矩阵是输入矩阵和卷积核矩阵重合部分的内积，即卷积特征矩阵每个位置上的值是_重合部分两个矩阵相应元素的乘积之和_。因此，卷积特征矩阵称为特征图。计算过程如图所示：![](https://tva1.sinaimg.cn/large/008i3skNgy1gz9g5h7hf6j30u0140ahh.jpg)
显然卷积特征矩阵比原来的输入矩阵维数低，如果希望得到的卷积特征矩阵维数和原来的输入矩阵一样，可以在输入矩阵四周用0填充，扩大输入矩阵的维数。

### 卷积神经网络的局部连接
CNN受生物学视觉系统结构启发，由每个映射面上的神经元共享权值，因而减少了网络自由参数的个数。
一般认为，人对外界的认知是从局部到全局的，而图像的空间联系也是局部的像素较为紧密，距离较远的像素相关性较弱。视觉的神经元就是局部接收信息的，这些神经元只接受某些特定区域刺激的相应。
因而，每个神经元不是对全局图像进行感知而只对局部进行感知吗，然后在最高层将局部信息综合起来得到全局信息。这样就可以减少神经元之间的连接数，从而减少神经网络需要训练的权值参数的个数。
### 卷积神经网络的权值共享
如果隐含层的每一个神经元只和10×10个像素连接，也就是说每一个神经元存在100个连接权值参数，若将每个神经元的100个参数设置成相同的，那就只有100个参数，_不管隐层的神经元个数有多少_，两层间的连接都只有100个参数，这就是卷积神经网络的权值共享。
> 上述讨论未考虑每个神经元的偏置部分，所以共享权值个数需要加1，这也是同一种滤波器所共享的。
权值共享隐含的原理是：图像的一部分统计特性与其他部分是一样的。意味着在这一部分学习的特征也能用在另一部分上，所以对于这个图像的所有位置都能使用同样的学习特征。
### 卷积神经玩过的多卷积核
卷积神经网络的多卷积核就是添加多个不同卷积核，分别提取不同的特征。每个卷积核都会将原图像生成另一幅图像，生成的不同的图像可以看作一张图像的不同通道。
### 卷积神经网络的池化
通过卷积获得特征之后，如果直接利用这些特征训练分类器，计算量是非常大的，为了解决这个问题，需要对不同位置的特征进行_聚合统计_。例如，可以计算图像一个区域上的某个特定特征的平均值，这些聚合的统计特征不仅具有_低得多的维度_，同时还会改善结果（不容易过拟合）。这种聚合操作就叫做池化（pooling），有时采用平均池化或者最大池化方法。
卷积神经网络在池化层_丢失大量信息_，从而降低了空间分辨率，导致了对于输入
微小变化，其输出几乎是不变的。

## 生成对抗网络
深度学习的模型可以大致分为判别模型和生成模型。目前，深度学习取得的成果主要集中在判别模型。判别模型是将一个高维的感官输入映射为一个类别标签。研究生成模型不多的主要原因是在对于深度神经网络使用最大似然估计时遇到了麻烦的概率计算问题，而GAN的提出则巧妙地绕过了这个问题。
生成对抗网络中有两个角色：生成器和判别器。生成对抗网络的基本原理类似于“左右互搏”之术，生成器类似于左手扮演攻方，判别器类似于右手扮演守方，生成器和判别器在对抗中提高各自的生成和判别能力，类似于造价币技术和验钞技术之间的关系。
### 生成对抗网络的基本原理
著名物理学家Richard指出，_要想真正理解一样东西，必须能够把它创造出来_。因此，要想令机器理解现实世界并基于此进行推理与创造，从而实现真正的人工智能，必须使机器能够通过观察现实世界的样本学习其**内在统计规律**，并基于此_生成类似样本_。这种能够反映数据内在概率分布规律并生成全新数据的模型，称为**生成式模型**。
Goodfellow等于2014年提出了一种新型生成式模型——生成对抗网络（generative adversarial network,GAN)，通过使用对抗训练机制对两个神经网络进行训练，经_随机梯度下降_实现优化，既避免了反复应用马尔科夫链学习机制所带来的的配分函数计算，也无须变分下限或近似推断，从而大大提高了利用效率。
生成方法是机器学习方法的一个重要分支，涉及对数据显式或隐式变量的分布假设和对分布参数的学习，基于学习得到的模型采样出新样本。传统生成模型往往采用_最大似然函数_作为目标函数，而GAN则在生成模型之外_引入一个判别模型_，通过两者之间的对抗训练达到优化目的。
在二元零和博弈中，博弈双方的利益之和为零或一个常数。基于这个思想，GAN的框架中包含一堆相互对抗的模型——判别器和生成器，判别器的目的是_正确区分真实数据和生成数据_，从而最大化判别准确率；生成器则是_尽可能逼近真实数据的潜在分布_。为了在博弈中胜出，二者需不断提高各自的判别能力和生成能力，优化的目标就是寻找两者的**纳什均衡**。
### 生成对抗网络的结构
GAN结构如图所示![](https://tva1.sinaimg.cn/large/008i3skNgy1gz9g5m4upmj30u0140k0r.jpg)，生成器的输入是一个来自常见概率分布的**随机噪声矢量**z，输出是计算机生成的伪数据。判别器的输入是图片x，x可能采样于真实数据，_也可能采样于生成数据_；判别器的输出是一个标量，用来代表x是真实图片的概率。
判别器和生成器不断优化，当判别器无法正确区分数据来源时，可以认为生成器捕捉到了真实数据样本的分布。
### 生成对抗网络的训练
GAN的训练过程包括两个相互交替的阶段：一个是固定生成网络，用来训练判别网络；另一个是固定判别网络，用来训练生成网络。_两个网络相互对抗的过程就是各自网络参数不断调整的过程，而参数的调整过程就是学习过程。_
1. 固定生成网络训练判别网络
在训练判别网络时，通过不断给它输入两种类别的图片并标注不同的分值。将生成图片和真实图片组成一个_二分类_的数据集，训练判别网络，如果输入的图片来自真实数据集，则输出1，否则输出0，通过这样的训练来提高判别网络的甄别能力，同时为生成网络进一步的训练提供信息。
2. 固定判别网络训练生成网络
持续地在潜空间中生成一些_随机数据_，用生成网络将这些数据变换为生成图片，分值越高说明图片越逼真。生成网络_利用分值信息调整网络参数_后，使得后面生成出来的图片更接近实际图片。