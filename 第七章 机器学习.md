# 第七章 机器学习
至今，还没有统一的机器学习定义，而且也很难给出一个公认的和准确的定义。从字面上理解，机器学习就是让机器能像人一样**具有学习能力**。机器学习领域奠基人Mitchell认为机器学习是计算机科学和统计学的交叉，同时也是人工智能和数据科学的核心。他给出机器学习的经典定义为_利用经验来改善计算机系统自身的性能_，经验对应于历史数据，计算机系统对应于机器学习模型，而性能则是模型对新数据的处理能力。
更近一步说，机器学习的根本任务是数据的智能分析和建模，进而从数据里挖掘出有用的价值。
机器学习是建立在**数据建模**的基础上的，因此数据是进行机器学习的基础。
我们把所有数据的集合称为**数据集**，其中每条记录称为一个**样本**，样本在某方面的表现或性质称为**属性或特征**，每个样本的特征对应特征空间中的一个坐标向量，称为**特征向量**。
机器学习任务的目标就是从数据中学习出相应的**模型**，而模型可以从数据中学习出如何判断不同样本的各个特征。
## 监督学习
监督学习是机器学习最重要的一类方法，占据了绝大部分的机器学习算法。
监督学习就是在_已知输入和输出的情况下_，训练出一个模型，将输入**映射**到输出。
机器学习的过程就是利用算法建立输入变量好输出变量函数关系的过程，在这个过程中机器不断通过训练输入来指导算法不断改进。
> 如果输出的结果不正确，那么这个错误结果与期望正确的结果之间的误差将作为纠正信号传回到模型，以纠正模型的改进。
### K-近邻算法
K-近邻算法（K-nearest neighbors,KNN)是最简单的机器学习分类算法之一，适用于多分类问题。
简单来说，其核心思想就是“排队”，给定训练集，对于待分类的样本点，计算待预测样本和训练集中所有数据点的**距离**，将距离从小到大取K个，则哪个类别在前K个数据点中的数量最多，就认为待预测样本属于该类别。
> 一句古话可以形象地说明：“近朱者赤，近墨者黑。”
KNN算法最大的优点是简单且容易实现，支持多分类，并且不需要进行训练，可以直接用训练数据来实现分类。
最主要的缺点是对参数的选择很敏感，比如不同的K值可能会产生不一样的结果。另一个缺点是计算量大，每次分类都要计算未知数据与所有训练样本的距离，因此在实际应用中被采用的不多。
### 决策树
决策树是一类常见的监督学习方法，代表的是对象属性与对象值之前的映射关系。
顾名思义，决策树是基于树结构来进行决策的，一颗决策树一般包含一个根节点、若干个内部节点、若干个叶节点，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。
![](https://tva1.sinaimg.cn/large/008i3skNgy1gz9g4fb8hsj30u0140dli.jpg)
决策树学习的目的是产生一棵泛化能力强（即处理未见实例能力强)的决策树，其基本流程遵循“分而治之”的策略。
决策树学习的关键是如何选择最优划分属性。一般而言，随着划分过程不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即“纯度”越来越高。
### 支持向量机
support vector machine(SVM)于1995年正式发表，由于其严格的理论基础以及在诸多分类任务中显示出的卓越性能，很快称为机器学习的主流技术，并掀起“统计学习”的高潮。
给定一组训练实例，每个训练实例被标记为属于两个类别中的一个或另一个，SVM训练算法通过寻求**结构化风险最小**来提高学习机的**泛化**能力，实现经验风险和置信范围的最小化，建立一个将新的实例分配给两个类别之一的模型，从而达到_在统计样本量较小的情况下亦能获得良好统计规律的目的。_
SVM模型将实例表示为空间中的点，这样映射就使得单独类别的实例被尽可能大地间隔（margin）分开，将新的实例映射到同一空间，然后基于它们落在间隔的哪一侧来预测所属的类别。
支持向量机的目标就是通过求解超平面将**不同属性**的点分开。
> 超平面是n维空间中的n-1维的子空间，在二维空间中的例子就是一条直线
一般而言，一个点距离超平面的远近可以表示为分类预测的确信或准确程度。为了提高分类的置信度，我们希望所选择的超平面能够_最大化_间隔值。这就是SVM算法的基础，即 **最大间隔原则**。
距离超平面最近的几个训练样本被称为**支持向量**，两个异类支持向量到超平面的距离之和被称为**间隔**，支持向量机的目标就是找到具有最大间隔的**划分超平面**。
在现实任务中原始样本空间内也许并不存在一个能正确划分两类样本的超平面，为了解决这类问题提出很多解决办法，其中最重要的方法是**核方法**。这种方法通过选择一个核函数，_将数据映射到高维空间_，使得在高维属性空间中有可能训练数据实现超平面的分割，避免了在原输入空间中进行非线性曲面分割计算，以解决在原始空间中_线性不可分_的问题。
由于核函数的良好性能，计算量只和支持向量的**数量**有关，而独立于空间的维度；而且在处理高维输入空间的分类时，这样的非线性拓展在计算量上并没有显著增加。因此核方法在目前机器学习任务中有非常广泛的应用，尤其在解决_线性不可分_问题当中。

## 无监督学习
顾名思义，无监督学习就是不受监督的学习。同监督学习建立在人类标注数据的基础上不同，无监督学习不需要人类进行数据标注，而是通过模型不断地自我认知、自我巩固，最后通过自我归纳来实现其学习过程。
同监督学习相比，无监督学习最大的优势是不再需要_大量的标注数据_。
实际上，无监督学习更接近人类的学习方式。比如，当一个婴儿开始接触世界时，父母指着一只猫告诉他这是“猫”，但是接下来在遇到不同的猫时，父母并不会一直告诉他这是猫，婴儿会不断地自我发现、学习、调整自己对猫的认识，从而最终理解并认识什么是“猫”。
### 聚类
聚类是无监督学习中_最重要_的一类算法。在聚类算法中，训练样本的标记信息是未知的，给定一个由样本点组成的数据集，数据聚类的目标是通过对无标记训练样本的学习来揭示数据的内在性质和规律，将样本点划分为若干类，使得属于同一类的样本点非常相似而不同类的样本点不相似。
聚类既能作为一个_单独过程_用于寻找数据内在的分布结构，也可作为分类等其他学习任务的_前驱过程_，为进一步的数据分析提供基础。
简单来说，聚类是将样本集分为若干_互不相交_的子集，即**样本簇**。聚类算法的目标是使同一簇的样本尽可能彼此相思，即具有较高的**类内相似度**；同时不同簇的样本尽可能不同，即簇间的相似度低。
目前使用最广泛的聚类方法是**K-均值算法**，其思想和实现都比较简单。对于给定样本集合，K-均值算法的目标是使得聚类簇内的平方误差最小化。K-均值算法的求解通常采用**贪心策略**，通过迭代方法实现。
K-均值算法时间复杂度近于线性，适合挖掘大规模数据集，但是由于损失函数是非凸函数，意味着不能保证取得的最小值是全局最小值。
需要注意的是，K-均值算法对参数的选择比较敏感，也就是说不同的初始位置或者类别数量的选择往往会导致不同的结果。
### 自编码器
自编码器是利用神经网络实现无监督学习的一种典型方式，包括编码器和解码器两个典型部分。输入数据经过隐层的编码和解码到达输出层时，使输出的结果尽量与输入数据保持一致。
我们通过编码器将输入的样本映射到一个特征空间上得到样本的**隐藏特征表达**，进而通过解码器将该特征向量进行解码。这样做的好处是隐层能抓住输入数据的_特点_，_使其特征保持不变_。在实际过程中，我们需要设计相应的**约束和损失函数**来决定数据的哪些部分应该_优先复制_，从而学习到数据当中的_有用特征_。
自编码器的应用主要有两个方面。一是数据去噪，通过引入合适的**损失函数**，使得模型可以_在受损的输入情况下_依然可以获得良好特征表达。二是数据降维，通过对隐特征加上适当的_维度和稀疏性约束_，使得自编码器可以学习到低维的数据投影。比如输入层由100个神经元，隐层有50个神经元，输出层有100个神经元，通过自编码器算法，我们只用隐含层的50个神经元就找到了100个输入层数据的特点，从而_保证了输出数据和输入数据大体一致_，实现降维目标。

## 弱监督学习
弱监督学习不仅可以降低人工标记的工作量，同时也可以引入人类的监督信息，在很大程度上提高无监督学习的性能。
同监督学习不同，弱监督学习中的数据标签允许是不完全的，即训练集中只有一部分数据是有标签的，其余甚至绝大部分数据是没有标签的；或者说机器学习的信号并不是直接指定给模型，而是通过一些引导信息间接传递给机器学习模型。总之，弱监督学习涵盖的范围很广泛，只要_标注信息不完全、不确切、不精确的标记学习都可以看作是弱监督学习_。本节选取半监督学习、迁移学习、强化学习来介绍弱监督学习的概念。
### 半监督学习
半监督学习是一种典型的弱监督学习方法，在半监督学习当中，我们通常只拥有少量有标注数据，虽然这些标注数据不足以训练出好的模型，但同时我们拥有_大量为标注数据_可以使用，我们可以通过充分地利用_少量的有监督数据_和_大量的无监督数据_来改善算法性能。因此，半监督学习可以_最大限度_地发挥数据的价值，使机器学习模型从体量巨大、结构繁多的数据中_挖掘出隐藏的规律_，也因此称为近年机器学习领域比较活跃的研究方向，广泛用于社交网络分析、文本分类、计算机视觉等领域。
在半监督学习中，基于图的半监督学习方法被广泛应用，该方法将_数据样本间的关系_映射为一个_相似度图_，其中，图的节点表示数据点，图的边被赋予相应权重，_代表数据点之间的相似度_。
对于无标记样本的识别，可以通过图上标记信息传播的方法实现，节点之间的相似度越大，标签越容易传播。在传播过程中，保持已标注数据的标签不变，使其像一个源头把标签传向未标注节点。
基于图的半监督学习方法简单有效，同时还可以针对实际问题灵活定义数据之间的相似性，具有很强的灵活性。尤其需要指出的是，该方法具有坚实的数学基础作保障，通常可以得到闭式最优解，因此具有广泛的适用范围。
### 迁移学习
迁移学习是另一类比较重要的弱监督学习方法，侧重于将已经学习过的知识迁移应用到新的问题中。
在迁徙学习中，通常称有知识和量数据标注的领域为**源域**，是我们要迁移的对象；而把最终要赋予知识和标注的对象称为**目标域**。迁移学习的核心目标就是将知识从源域迁移到目标域。
目前，迁移学习主要通过三种方式实现：
1. 样本迁移
即在源域中找到与目标域相似的数据并赋予其更高的权重，从而完成从源域到目标域的迁移。这种方法的好处是简单且容易实现，但是权重和相似度的选择往往高度依赖经验，使得算法的可靠性降低。
2. 特征迁移
其核心思想是通过特征变换，将源域和目标域的特征映射到同一个特征空间中，然后再用经典的机器学习方法来求解。这种方法的好处是对大多数方法都使用且效果较好，但是在实际问题中求解难度通常较大。
3. 模型迁移
模型歉意是目前最主流的方法，这种方法假设源域和目标域共享模型参数，将之前在源域中通过大量数据训练好的模型应用到目标域上。
比如，我们在一个千万量级的标注样本集上训练得到的模型，在处理一个新领域的任务中，可以直接利用原来的模型加上目标域的记完账标注样本进行微调，就可以得到很高的精度。这种方法可以很好地利用模型之间的相似度，具有广阔的应用前景。
_迁移学习可以充分利用既有模型的知识，使机器学习模型在面临新的任务时只需要进行少量的**微调**即可完成相应的任务，具有重要的应用价值。_
### 强化学习
强化学习也是弱监督学习的一类典型算法，其算法理论可追溯到二十世纪七八十年代，但却是在最近因为ALphaGo程序利用强化学习算法打败李世石这件事才引发广泛关注。
与监督学习不同，强化学习需要通过_尝试_来发现各个动作产生的结果，而没有训练数据告诉机器应当做哪个动作，但是可以通过设置合适的奖励函数，使机器学习模型在奖励函数的引导下自主学习出相应策略。
强化学习的目标就是研究在与环境交互过程中，如何学习到一种行为策略以最大化得到的累计奖赏。
需要指出的是，强化学习通常由两种不同的策略，一是探索，也就是尝试不同的事情，看他们是否会获得比之前更好的回报；二是利用，也就是尝试过去经验当中最有效的行为。探索会面临风险，而一昧地利用就无法进步，探索和利用的矛盾是强化学习要解决的一个难点问题。
还有一点，强化学习允许结果奖励信号的反馈有**延时**，即可能需要经过很多步骤才能得到最后的反馈，而监督学习不同，它没有奖励函数，就像一个 导师在旁边，当算法做了错误的选择就会立刻纠正，不存在延时。

